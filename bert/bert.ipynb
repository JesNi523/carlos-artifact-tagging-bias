{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAINING SECTION -- YOU ONLY NEED TO RUN IF YOU DONT HAVE MODEL FOLDER**\n",
    "\n",
    "CHANGE ALL PATHS FIRST: CHANGE DATA READING PATH AND CHANGE DATA SAVING PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of      ObjectID                               Title  \\\n",
      "0           0  Engaged Corner Capital with Leaves   \n",
      "1           1                              Trivet   \n",
      "2           2                               Point   \n",
      "3           3                    A Refugee Family   \n",
      "4           6        Perfume Vessel (Amphoriskos)   \n",
      "..        ...                                 ...   \n",
      "96        170                          Copper Ore   \n",
      "97        171         Three Pages from a Bestiary   \n",
      "98        173                                Ball   \n",
      "99        175       Bell with Demotic Inscription   \n",
      "100       176                    Tanagra Figurine   \n",
      "\n",
      "                                             TextEntry  Subjective  Gender  \\\n",
      "0                  No provenance information in files.           0       0   \n",
      "1    \"The Carlos Museum's collection of ancient Ame...           0       0   \n",
      "2    Codes used specifically to disguise the conten...           0       0   \n",
      "3    Inhouse exhibition with loans from the Tampa M...           0       0   \n",
      "4    \"Schatten Gallery, EUMAA\"\\n\\n\"Schatten Gallery...           0       0   \n",
      "..                                                 ...         ...     ...   \n",
      "96   In between cycles of time Vishnu floats in dee...           0       0   \n",
      "97   February 1993 descriptor moved.\\n\\nDonor acqui...           0       0   \n",
      "98   Previous identification as a game ball questio...           0       0   \n",
      "99   Study of Egyptian Collection:\\r\\n\"1921.49  Bro...           0       0   \n",
      "100  February 1993 descriptor moved.\\n\\nNo provenan...           0       0   \n",
      "\n",
      "     Jargon  Social Subjective Label Gender Label  \\\n",
      "0         0     0.0               \"\"           \"\"   \n",
      "1         0     0.0               \"\"           \"\"   \n",
      "2         1     0.0               \"\"           \"\"   \n",
      "3         1     0.0               \"\"           \"\"   \n",
      "4         1     1.0               \"\"           \"\"   \n",
      "..      ...     ...              ...          ...   \n",
      "96        1     0.0               \"\"           \"\"   \n",
      "97        0     0.0               \"\"           \"\"   \n",
      "98        0     1.0               \"\"           \"\"   \n",
      "99        0     0.0               \"\"           \"\"   \n",
      "100       0     0.0               \"\"           \"\"   \n",
      "\n",
      "                                          Jargon Label  \\\n",
      "0                                                   \"\"   \n",
      "1                                                   \"\"   \n",
      "2    \"Etowah or Etowah-related provenance\", \"TMS\", ...   \n",
      "3                                              MHDPM   \n",
      "4    Amphoriskos, Oinochoe, Pelike, Kylix, Lekythos...   \n",
      "..                                                 ...   \n",
      "96   cosmic ocean,\\n\\nAnanta,\\n\\ncyclical time,\\n\\n...   \n",
      "97                                                  \"\"   \n",
      "98                                                  \"\"   \n",
      "99                                                  \"\"   \n",
      "100                                                 \"\"   \n",
      "\n",
      "                                          Social Label  \n",
      "0                                                   \"\"  \n",
      "1                                                   \"\"  \n",
      "2                                                   \"\"  \n",
      "3                                                   \"\"  \n",
      "4                                      \"Head of black\"  \n",
      "..                                                 ...  \n",
      "96                                                  \"\"  \n",
      "97                                                  \"\"  \n",
      "98   Care of children or others unable to be left b...  \n",
      "99                                                  \"\"  \n",
      "100                                                 \"\"  \n",
      "\n",
      "[101 rows x 11 columns]>\n",
      "No provenance information in files.\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 1. 0.]\n",
      " [1. 0. 1. 0.]\n",
      " [1. 0. 1. 1.]\n",
      " [1. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 1. 0.]\n",
      " [1. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 1. 0.]\n",
      " [1. 1. 1. 0.]\n",
      " [1. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 1. 0.]\n",
      " [0. 1. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 1. 1.]\n",
      " [1. 0. 1. 0.]\n",
      " [1. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import transformers, torch\n",
    "\n",
    "df = pd.read_csv('/Users/raasikh/Documents/Coding/ai.xperience/carlos-artifact-tagging-bias/carlos_data/clean_data_annotated_v2.csv', encoding='latin1')\n",
    "df = df.head(101)\n",
    "\n",
    "print(df.head)\n",
    "\n",
    "descs = df['TextEntry'].tolist()\n",
    "bias_clas = df[['Subjective', 'Gender', 'Jargon', 'Social']].values\n",
    "\n",
    "print(descs[0])\n",
    "\n",
    "print(bias_clas)\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the texts\n",
    "inputs = tokenizer(descs, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BiasDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.inputs.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "dataset = BiasDataset(inputs, bias_clas)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "model = transformers.BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raasikh/Documents/Coding/ai.xperience/carlos-artifact-tagging-bias/myenv/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6035284570285252\n",
      "Epoch 2, Loss: 0.5183091163635254\n",
      "Epoch 3, Loss: 0.4555529398577554\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = BCEWithLogitsLoss()\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=batch['input_ids'],\n",
    "                        attention_mask=batch['attention_mask'],\n",
    "                        labels=batch['labels'])\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/raasikh/Documents/Coding/ai.xperience/carlos-artifact-tagging-bias/bert/model/tokenizer_config.json',\n",
       " '/Users/raasikh/Documents/Coding/ai.xperience/carlos-artifact-tagging-bias/bert/model/special_tokens_map.json',\n",
       " '/Users/raasikh/Documents/Coding/ai.xperience/carlos-artifact-tagging-bias/bert/model/vocab.txt',\n",
       " '/Users/raasikh/Documents/Coding/ai.xperience/carlos-artifact-tagging-bias/bert/model/added_tokens.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('/Users/raasikh/Documents/Coding/ai.xperience/carlos-artifact-tagging-bias/bert/model')\n",
    "tokenizer.save_pretrained('/Users/raasikh/Documents/Coding/ai.xperience/carlos-artifact-tagging-bias/bert/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREDICTION**\n",
    "\n",
    "Run the first code block only if you have the model folder and have NOT done training above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# CHANGE TO YOUR PATH OF MODEL FOLDER\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('/Users/raasikh/Documents/Coding/ai.xperience/carlos-artifact-tagging-bias/bert/model')\n",
    "model = transformers.BertForSequenceClassification.from_pretrained('/Users/raasikh/Documents/Coding/ai.xperience/carlos-artifact-tagging-bias/bert/model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2832, 0.2683, 0.5392, 0.2065],\n",
      "        [0.2868, 0.2462, 0.5434, 0.2164]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# Whatever you want it to predict\n",
    "validation_texts = [\"Harsh, uncomfortably brilliant, usually reflected light.  W. April 1993 descriptor moved.\", \n",
    "                    \"December 1992 lead-in term added. January 1991 alternate term added. Object fumigated in Orkin's Piedmont vault with Vikane in 1994\"]\n",
    "\n",
    "# Tokenize validation texts\n",
    "validation_inputs = tokenizer(validation_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**validation_inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.sigmoid(logits)\n",
    "\n",
    "# Print predictions\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
