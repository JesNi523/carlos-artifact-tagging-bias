{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **-------------------------------------------------------------**\n",
    "\n",
    "# **-------------------------------------------------------------**\n",
    "\n",
    "# **-------------------------------------------------------------**\n",
    "\n",
    "# **-------------------------------------------------------------**\n",
    "\n",
    "# **-------------------------------------------------------------**\n",
    "\n",
    "# **-------------------------------------------------------------**\n",
    "\n",
    "# **-------------------------------------------------------------**\n",
    "\n",
    "# **-------------------------------------------------------------**\n",
    "\n",
    "# **-------------------------------------------------------------**\n",
    "\n",
    "# **-------------------------------------------------------------**\n",
    "\n",
    "# **-------------------------------------------------------------**\n",
    "\n",
    "# **-------------------------------------------------------------**\n",
    "\n",
    "# **-------------------------------------------------------------**\n",
    "\n",
    "# **-------------------------------------------------------------**\n",
    "\n",
    "# **-------------------------------------------------------------**\n",
    "\n",
    "# **-------------------------------------------------------------**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAINING SECTION -- YOU ONLY NEED TO RUN IF YOU DONT HAVE MODEL FOLDER**\n",
    "\n",
    "CHANGE ALL PATHS FIRST: CHANGE DATA READING PATH AND CHANGE DATA SAVING PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas transformers torch datasets numpy\n",
    "#!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['ObjectID', 'Description', 'Subjective', 'Gender', 'Jargon', 'Social'],\n",
      "        num_rows: 642\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['ObjectID', 'Description', 'Subjective', 'Gender', 'Jargon', 'Social'],\n",
      "        num_rows: 80\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['ObjectID', 'Description', 'Subjective', 'Gender', 'Jargon', 'Social'],\n",
      "        num_rows: 81\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import transformers, torch\n",
    "\n",
    "\"\"\"\n",
    "Remove uneecessay new lines\n",
    "remove duplicates within one cell\n",
    "Remove links?\n",
    "use keywords to make sure that correct classification is done\n",
    "make validation set more class balanced, i.e. equal num of examples for each category\n",
    "\"\"\"\n",
    "# load data and rename TextEntry column\n",
    "df = pd.read_excel(\"../../../carlos_data/preprocessed_data_v1.xlsx\")\n",
    "df = df.rename(columns={\"TextEntry\":\"Description\"})\n",
    "\n",
    "# convert data to dictionary\n",
    "data = df.to_dict(\"records\")\n",
    "\n",
    "# Split the data into train and validation and test sets\n",
    "from sklearn.model_selection import train_test_split \n",
    "train_dict, test_dict = train_test_split(data, test_size=0.20, random_state=42)\n",
    "test_dict, validation_dict = train_test_split(test_dict, test_size=0.50, random_state=42)\n",
    "\n",
    "# Create Dataset objects\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_list(train_dict)\n",
    "test_dataset = Dataset.from_list(test_dict)\n",
    "validation_dataset = Dataset.from_list(validation_dict)\n",
    "\n",
    "# Create DatasetDict\n",
    "from datasets import DatasetDict\n",
    "dataset = DatasetDict({\n",
    "\t\"train\": train_dataset,\n",
    "\t\"test\": test_dataset,\n",
    "\t\"validation\": validation_dataset\n",
    "})\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Subjective', 'Gender', 'Jargon', 'Social']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# credit: https://github.com/NielsRogge/Transformers-Tutorials\n",
    "labels = [label for label in dataset[\"train\"].features.keys() if label not in [\"ObjectID\", \"Description\"]]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# credit: https://github.com/NielsRogge/Transformers-Tutorials\n",
    "def preprocess_data(data):\n",
    "\n",
    "\t# save the given batch of descs\n",
    "\tdescs = data[\"Description\"]\n",
    "\n",
    "\t# encode them using bert tokenizer\n",
    "\tencoding = tokenizer(descs, padding=True, truncation=True, max_length=512)#.to(\"mps\")\n",
    "\n",
    "\t# create numpy array (no need to convert T/F to 0/1 since we annotated that way)\n",
    "\t# MATRIX FORMAT:\n",
    "\t# |---------------------------------\n",
    "\t# | bias   | bias1 bias2 bias3 bias4\n",
    "\t# |--------+------------------------\n",
    "\t# | desc0  |   1     0     1     0\n",
    "\t# | desc1  |   0     1     0     1\n",
    "\t# | desc2  |   0     1     0     0\n",
    "\t# | ...    |  ...   ...   ...   ...\n",
    "\t# \n",
    "\t# Convert integers to float and data to an NDarray\n",
    "\tsubjective = np.array(data[\"Subjective\"], dtype=float)\n",
    "\tgender = np.array(data[\"Gender\"], dtype=float)\n",
    "\tjargon = np.array(data[\"Jargon\"], dtype=float)\n",
    "\tsocial = np.array(data[\"Social\"], dtype=float)\n",
    "\t# Stack the arrays column-wise to form a 2D array (matrix)\n",
    "\tlabels_matrix = np.stack((subjective, gender, jargon, social), axis=1)\n",
    "\n",
    "\t\n",
    "\t# # Credit ChatGPT\n",
    "\t# # Validate the data stacking by comparing 3 random indices\n",
    "\t# import random\n",
    "\t# for _ in range(3):\n",
    "\t# \tidx = random.randint(0, len(subjective) - 1)\n",
    "\t# \tdataset_labels = [data[\"Subjective\"][idx], data[\"Gender\"][idx], data[\"Jargon\"][idx], data[\"Social\"][idx]]\n",
    "\t# \tmatrix_labels = labels_matrix[idx].tolist()\n",
    "\t# \tassert dataset_labels == matrix_labels, f\"Mismatch at index {idx}: {dataset_labels} != {matrix_labels}\"\n",
    "\t# \tprint(f\"Index {idx} matches: {dataset_labels}\")\n",
    "\n",
    "\n",
    "\t# FORMAT OF var encoding of type BatchEncoding (the length of the vals of each key \n",
    "\t# equal the num of descs/objects in given batch):\n",
    "\t# input_ids: [101, 1030, 4748, 7229, 1035, ...], ...\n",
    "\t# token_type_ids: [0, 0, 0, 0, 0, ...], ...\n",
    "\t# attention_mask: [1, 1, 1, 1, 1, ...], ...\n",
    "\t# labels: [1.0, 1.0, 0.0, 0.0], ...\n",
    "\tencoding[\"labels\"] = labels_matrix.tolist()\n",
    "\n",
    "\treturn encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = dataset.map(preprocess_data, batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] banner stone ( spearthrower weight ) location verified, inventory 2001 location verified, inventory 2003. turner inventory, april 2004. location verified, inventory 9, winter 2005. location verified, inventory 2001 location verified, inventory 2003. turner inventory, april 2004. location verified, inventory 9, winter 2005. note : updated in june 2021 with information from 1984 accession worksheet. a february 14, 1997 file memo from registrar lori iliff discussed how sources and provenance were determined for objects that were believed to have an etowah or etowah - related provenance. as x. 0250. 002 is not believed to have originated from etowah, it was not discussed in this memo. however, to document the museum records for this piece, the accession log i ( green - blue colored ledger ) does not note a provenance for x. 0250. 002 and notes the source as ferguson. x. 0250. 001 and. 002 share one specimen record that does not note a provenance and notes ferguson as source. the 1984 accession worksheet in the object file notes the source as ferguson and provenance as southeast u. s. note : updated in june 2021 with information from 1984 accession worksheet. a february 14, 1997 file memo from registrar lori iliff discussed how sources and provenance were determined for objects that were believed to have an etowah or etowah - related provenance. as x. 0250. 002 is not believed to have originated from etowah, it was not discussed in this memo. however, to document the museum records for this piece, the accession log i ( green - blue colored ledger ) does not note a provenance for x. 0250. 002 and notes the source as ferguson. x. 0250. 001 and. 002 share one specimen record that does not note a provenance and notes ferguson as source. the 1984 accession worksheet in the object file notes the source as ferguson and provenance as southeast u. s. the exhibition organizer, halsey institute, has made a series of materials available via dropbox including copies of the videos, info about projectors, info about the pool assembly, and a checklist / crate list : https : / / www. dropbox. com / sh / eiphoh1fm6xr2iw / aaaocu4qpcvyjuftkxvbeqooa? dl = 0 the exhibition organizer, halsey institute, [SEP]\n",
      "[0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# see example\n",
    "print(tokenizer.decode(encoded_dataset[\"train\"][0][\"input_ids\"]))\n",
    "print(encoded_dataset[\"train\"][0][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset a standard torch dataset by converting to tensors (and more?)\n",
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)#.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add Warmup?\n",
    "Experiment with learning rate\n",
    "experiment with batch size\n",
    "experient with gradient_accumulation_steps\n",
    "another metric?\n",
    "Choose another optimizer: RMSprop, SGD...\n",
    "Increase the learning rate by default and then use the callback ReduceLROnPlateau\n",
    "\"\"\"\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "num_epochs = 50\n",
    "batch_size = 16\n",
    "lr = 2e-5\n",
    "metric_name = \"f1\"\n",
    "decay = 0.01\n",
    "\n",
    "model_path = \"./model\"\n",
    "tokenizer_path = f\"./tokenizer\"\n",
    "logs_path = f\"./logs\"\n",
    "\n",
    "with open('../../../hg_token.txt', 'r') as file:\n",
    "\thg_token = file.read()\n",
    "\n",
    "\n",
    "# args for training the model\n",
    "# save the model every epoch and choose the best performing epoch as the final version of the model\n",
    "args = TrainingArguments(\n",
    "\teval_strategy = \"epoch\",\n",
    "\tsave_strategy = \"epoch\",\n",
    "\tlearning_rate = lr,\n",
    "\tper_device_train_batch_size = batch_size,\n",
    "\tper_device_eval_batch_size = batch_size,\n",
    "\tnum_train_epochs = num_epochs,\n",
    "\tweight_decay = decay,\n",
    "\tload_best_model_at_end = True,\n",
    "\tmetric_for_best_model = metric_name,\n",
    "\tlogging_dir = logs_path,\n",
    "\toutput_dir = model_path,\n",
    "\t# use_mps_device = True,\n",
    "\t# use_cpu = False,\n",
    "\tlogging_steps = 1,\n",
    "\t# gradient_accumulation_steps=2,\n",
    "\thub_token = hg_token,\n",
    "\thub_model_id = \"raasikhk/carlos_bert_v2\",\n",
    "\t\n",
    ")\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "\n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "\t# first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "\tsigmoid = torch.nn.Sigmoid()\n",
    "\tprobs = sigmoid(torch.Tensor(predictions))\n",
    "\t# next, use threshold to turn them into integer predictions\n",
    "\ty_pred = np.zeros(probs.shape)\n",
    "\ty_pred[np.where(probs >= threshold)] = 1\n",
    "\t# finally, compute metrics\n",
    "\ty_true = labels\n",
    "\tf1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "\troc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "\taccuracy = accuracy_score(y_true, y_pred)\n",
    "\t# return as dictionary\n",
    "\tmetrics = {'f1': f1_micro_average,\n",
    "\t\t\t\t'roc_auc': roc_auc,\n",
    "\t\t\t\t'accuracy': accuracy}\n",
    "\treturn metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "\tpreds = p.predictions[0] if isinstance(p.predictions, \n",
    "\t\t\ttuple) else p.predictions\n",
    "\tresult = multi_label_metrics(\n",
    "\t\tpredictions=preds, \n",
    "\t\tlabels=p.label_ids)\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "\tmodel,\n",
    "\targs,\n",
    "\ttrain_dataset=encoded_dataset[\"train\"],\n",
    "\teval_dataset=encoded_dataset[\"validation\"],\n",
    "\ttokenizer=tokenizer,\n",
    "\tcompute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TrainOutput(global_step=2050, training_loss=0.06863384766890326, metrics={'train_runtime': 5019.4944, 'train_samples_per_second': 6.395, 'train_steps_per_second': 0.408, 'total_flos': 8446016541081600.0, 'train_loss': 0.06863384766890326, 'epoch': 50.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'eval_loss': 0.354525089263916,\n",
    " 'eval_f1': 0.6222222222222222,\n",
    " 'eval_roc_auc': 0.766304347826087,\n",
    " 'eval_accuracy': 0.7037037037037037,\n",
    " 'eval_runtime': 2.5143,\n",
    " 'eval_samples_per_second': 32.215,\n",
    " 'eval_steps_per_second': 2.386,\n",
    " 'epoch': 50.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorboard/compat/__init__.py\", line 42, in tf\n",
      "    from tensorboard.compat import notf  # noqa: F401\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ImportError: cannot import name 'notf' from 'tensorboard.compat' (/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorboard/compat/__init__.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/bin/tensorboard\", line 8, in <module>\n",
      "    sys.exit(run_main())\n",
      "             ^^^^^^^^^^\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorboard/main.py\", line 38, in run_main\n",
      "    main_lib.global_init()\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorboard/main_lib.py\", line 50, in global_init\n",
      "    if getattr(tf, \"__version__\", \"stub\") == \"stub\":\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorboard/lazy.py\", line 65, in __getattr__\n",
      "    return getattr(load_once(self), attr_name)\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorboard/lazy.py\", line 97, in wrapper\n",
      "    cache[arg] = f(arg)\n",
      "                 ^^^^^^\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorboard/lazy.py\", line 50, in load_once\n",
      "    module = load_fn()\n",
      "             ^^^^^^^^^\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorboard/compat/__init__.py\", line 45, in tf\n",
      "    import tensorflow\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorflow/__init__.py\", line 47, in <module>\n",
      "    from tensorflow._api.v2 import __internal__\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorflow/_api/v2/__internal__/__init__.py\", line 11, in <module>\n",
      "    from tensorflow._api.v2.__internal__ import distribute\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorflow/_api/v2/__internal__/distribute/__init__.py\", line 8, in <module>\n",
      "    from tensorflow._api.v2.__internal__.distribute import combinations\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorflow/_api/v2/__internal__/distribute/combinations/__init__.py\", line 8, in <module>\n",
      "    from tensorflow.python.distribute.combinations import env # line: 456\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorflow/python/distribute/combinations.py\", line 33, in <module>\n",
      "    from tensorflow.python.distribute import collective_all_reduce_strategy\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 32, in <module>\n",
      "    from tensorflow.python.distribute import mirrored_strategy\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 34, in <module>\n",
      "    from tensorflow.python.distribute.cluster_resolver import tfconfig_cluster_resolver\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorflow/python/distribute/cluster_resolver/__init__.py\", line 27, in <module>\n",
      "    from tensorflow.python.distribute.cluster_resolver.gce_cluster_resolver import GCEClusterResolver\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorflow/python/distribute/cluster_resolver/gce_cluster_resolver.py\", line 24, in <module>\n",
      "    from googleapiclient import discovery  # pylint: disable=g-import-not-at-top\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/site-packages/googleapiclient/discovery.py\", line 45, in <module>\n",
      "    from google.oauth2 import service_account\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/site-packages/google/oauth2/service_account.py\", line 77, in <module>\n",
      "    from google.auth import _service_account_info\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/site-packages/google/auth/_service_account_info.py\", line 20, in <module>\n",
      "    from google.auth import crypt\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/site-packages/google/auth/crypt/__init__.py\", line 41, in <module>\n",
      "    from google.auth.crypt import rsa\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/site-packages/google/auth/crypt/rsa.py\", line 20, in <module>\n",
      "    from google.auth.crypt import _cryptography_rsa\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/site-packages/google/auth/crypt/_cryptography_rsa.py\", line 27, in <module>\n",
      "    import cryptography.x509\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/site-packages/cryptography/x509/__init__.py\", line 7, in <module>\n",
      "    from cryptography.x509 import certificate_transparency, verification\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/site-packages/cryptography/x509/verification.py\", line 10, in <module>\n",
      "    from cryptography.x509.general_name import DNSName, IPAddress\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/site-packages/cryptography/x509/general_name.py\", line 12, in <module>\n",
      "    from cryptography.x509.name import Name\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/site-packages/cryptography/x509/name.py\", line 356, in <module>\n",
      "    class _RFC4514NameParser:\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/site-packages/cryptography/x509/name.py\", line 369, in _RFC4514NameParser\n",
      "    _STRING_RE = re.compile(\n",
      "                 ^^^^^^^^^^^\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/re/__init__.py\", line 227, in compile\n",
      "    return _compile(pattern, flags)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/re/__init__.py\", line 294, in _compile\n",
      "    p = _compiler.compile(pattern, flags)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/re/_compiler.py\", line 749, in compile\n",
      "    code = _code(p, flags)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/re/_compiler.py\", line 582, in _code\n",
      "    _compile(code, p.data, flags)\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/re/_compiler.py\", line 119, in _compile\n",
      "    _compile(code, av[2], flags)\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/re/_compiler.py\", line 128, in _compile\n",
      "    _compile(code, p, _combine_flags(flags, add_flags, del_flags))\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/re/_compiler.py\", line 119, in _compile\n",
      "    _compile(code, av[2], flags)\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/re/_compiler.py\", line 128, in _compile\n",
      "    _compile(code, p, _combine_flags(flags, add_flags, del_flags))\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/re/_compiler.py\", line 119, in _compile\n",
      "    _compile(code, av[2], flags)\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/re/_compiler.py\", line 128, in _compile\n",
      "    _compile(code, p, _combine_flags(flags, add_flags, del_flags))\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/re/_compiler.py\", line 176, in _compile\n",
      "    _compile(code, av, flags)\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/re/_compiler.py\", line 86, in _compile\n",
      "    charset, hascased = _optimize_charset(av, iscased, tolower, fixes)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/raasikh/.pyenv/versions/3.11.9/lib/python3.11/re/_compiler.py\", line 279, in _optimize_charset\n",
      "    charmap[i] = 1\n",
      "    ~~~~~~~^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# view logs (only needed for analysis)\n",
    "# !pip install tensorboard\n",
    "!tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"save_pretrained_model\")\n",
    "tokenizer.save_pretrained(\"save_pretrained_tokenizer\")\n",
    "# tokenizer = transformers.BertTokenizer.from_pretrained()\n",
    "# model = transformers.BertForSequenceClassification.from_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I'm happy I can finally train a model for multi-label classification\"\n",
    "\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
    "\n",
    "outputs = trainer.model(**encoding)\n",
    "\n",
    "logits = outputs.logits\n",
    "logits.shape\n",
    "\n",
    "# apply sigmoid + threshold\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(logits.squeeze().cpu())\n",
    "predictions = np.zeros(probs.shape)\n",
    "predictions[np.where(probs >= 0.5)] = 1\n",
    "# turn predicted id's into actual label names\n",
    "predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREDICTION**\n",
    "\n",
    "Run the first code block only if you have the model folder and have NOT done training above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0853, 0.0375, 0.3510, 0.0492],\n",
      "        [0.1108, 0.0471, 0.4768, 0.0637]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# Whatever you want it to predict\n",
    "validation_texts = [\"Harsh, uncomfortably brilliant, usually reflected light.  W. April 1993 descriptor moved.\", \n",
    "                    \"December 1992 lead-in term added. January 1991 alternate term added. Object fumigated in Orkin's Piedmont vault with Vikane in 1994\"]\n",
    "\n",
    "# Tokenize validation texts\n",
    "validation_inputs = tokenizer(validation_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**validation_inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.sigmoid(logits)\n",
    "\n",
    "# Print predictions\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF YOU DONT WANT TO TRAIN LOAD MODEL FROM HUGGINGFACE\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "import torch\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"raasikhk/carlos_bert_v2\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"raasikhk/carlos_bert_v2\")\n",
    "\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, top_k=None, device=device)\n",
    "\n",
    "texts = [\n",
    "    \"The artifact was created in the 19th century and is considered highly valuable.\",\n",
    "    \"This piece shows signs of heavy wear and might not be authentic.\"\n",
    "]\n",
    "\n",
    "# Make predictions\n",
    "predictions = pipe(texts)\n",
    "\n",
    "# Print the predictions\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predictions: {predictions[i]}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
