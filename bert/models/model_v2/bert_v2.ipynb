{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  !pip install pandas transformers torch datasets numpy openpyxl scikit-learn\n",
    "#  !pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['ObjectID', 'Description', 'Subjective', 'Gender', 'Jargon', 'Social'],\n",
      "        num_rows: 1280\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['ObjectID', 'Description', 'Subjective', 'Gender', 'Jargon', 'Social'],\n",
      "        num_rows: 160\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['ObjectID', 'Description', 'Subjective', 'Gender', 'Jargon', 'Social'],\n",
      "        num_rows: 161\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import transformers, torch\n",
    "\n",
    "\"\"\"\n",
    "Remove uneecessay new lines\n",
    "remove duplicates within one cell\n",
    "Remove links?\n",
    "use keywords to make sure that correct classification is done\n",
    "make validation set more class balanced, i.e. equal num of examples for each category\n",
    "\"\"\"\n",
    "# load data and rename TextEntry column\n",
    "df = pd.read_excel(\"../../../carlos_data/preprocessed_data_v2.xlsx\")\n",
    "df = df.rename(columns={\"TextEntry\":\"Description\"})\n",
    "\n",
    "# convert data to dictionary\n",
    "data = df.to_dict(\"records\")\n",
    "\n",
    "# Split the data into train and validation and test sets\n",
    "from sklearn.model_selection import train_test_split \n",
    "train_dict, test_dict = train_test_split(data, test_size=0.20, random_state=42)\n",
    "test_dict, validation_dict = train_test_split(test_dict, test_size=0.50, random_state=42)\n",
    "\n",
    "# Create Dataset objects\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_list(train_dict)\n",
    "test_dataset = Dataset.from_list(test_dict)\n",
    "validation_dataset = Dataset.from_list(validation_dict)\n",
    "\n",
    "# Create DatasetDict\n",
    "from datasets import DatasetDict\n",
    "dataset = DatasetDict({\n",
    "\t\"train\": train_dataset,\n",
    "\t\"test\": test_dataset,\n",
    "\t\"validation\": validation_dataset\n",
    "})\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Texts length distribution (number of words)')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df[\"Description\"], df[\"Social\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "text_lengths = [len(t.split()) for t in train_texts]\n",
    "ax = sns.histplot(data=text_lengths, kde=True, stat=\"density\")\n",
    "ax.set_title(\"Texts length distribution (number of words)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Subjective', 'Gender', 'Jargon', 'Social']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# credit: https://github.com/NielsRogge/Transformers-Tutorials\n",
    "labels = [label for label in dataset[\"train\"].features.keys() if label not in [\"ObjectID\", \"Description\"]]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# credit: https://github.com/NielsRogge/Transformers-Tutorials\n",
    "def preprocess_data(data):\n",
    "\n",
    "\t# save the given batch of descs\n",
    "\tdescs = data[\"Description\"]\n",
    "\n",
    "\t# encode them using bert tokenizer\n",
    "\tencoding = tokenizer(descs, padding=True, truncation=True, max_length=512)#.to(\"mps\")\n",
    "\n",
    "\t# create numpy array (no need to convert T/F to 0/1 since we annotated that way)\n",
    "\t# MATRIX FORMAT:\n",
    "\t# |---------------------------------\n",
    "\t# | bias   | bias1 bias2 bias3 bias4\n",
    "\t# |--------+------------------------\n",
    "\t# | desc0  |   1     0     1     0\n",
    "\t# | desc1  |   0     1     0     1\n",
    "\t# | desc2  |   0     1     0     0\n",
    "\t# | ...    |  ...   ...   ...   ...\n",
    "\t# \n",
    "\t# Convert integers to float and data to an NDarray\n",
    "\tsubjective = np.array(data[\"Subjective\"], dtype=float)\n",
    "\tgender = np.array(data[\"Gender\"], dtype=float)\n",
    "\tjargon = np.array(data[\"Jargon\"], dtype=float)\n",
    "\tsocial = np.array(data[\"Social\"], dtype=float)\n",
    "\t# Stack the arrays column-wise to form a 2D array (matrix)\n",
    "\tlabels_matrix = np.stack((subjective, gender, jargon, social), axis=1)\n",
    "\n",
    "\t\n",
    "\t# # Credit ChatGPT\n",
    "\t# # Validate the data stacking by comparing 3 random indices\n",
    "\t# import random\n",
    "\t# for _ in range(3):\n",
    "\t# \tidx = random.randint(0, len(subjective) - 1)\n",
    "\t# \tdataset_labels = [data[\"Subjective\"][idx], data[\"Gender\"][idx], data[\"Jargon\"][idx], data[\"Social\"][idx]]\n",
    "\t# \tmatrix_labels = labels_matrix[idx].tolist()\n",
    "\t# \tassert dataset_labels == matrix_labels, f\"Mismatch at index {idx}: {dataset_labels} != {matrix_labels}\"\n",
    "\t# \tprint(f\"Index {idx} matches: {dataset_labels}\")\n",
    "\n",
    "\n",
    "\t# FORMAT OF var encoding of type BatchEncoding (the length of the vals of each key \n",
    "\t# equal the num of descs/objects in given batch):\n",
    "\t# input_ids: [101, 1030, 4748, 7229, 1035, ...], ...\n",
    "\t# token_type_ids: [0, 0, 0, 0, 0, ...], ...\n",
    "\t# attention_mask: [1, 1, 1, 1, 1, ...], ...\n",
    "\t# labels: [1.0, 1.0, 0.0, 0.0], ...\n",
    "\tencoding[\"labels\"] = labels_matrix.tolist()\n",
    "\n",
    "\treturn encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1280/1280 [00:05<00:00, 241.67 examples/s]\n",
      "Map: 100%|██████████| 160/160 [00:00<00:00, 228.95 examples/s]\n",
      "Map: 100%|██████████| 161/161 [00:00<00:00, 272.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_data, batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] pendant in the form of a bird? no provenance information in files. date range comes from memo in accession lot that says witten collection was built between 1960 and 1985. location verified, inventory 2001 location verified, inventory 2003. location verified, inventory 9, fall 2004. october 1992 descriptor moved. describes buildings having engaged columns or pilasters along the sides and rear and freestanding columns along the front. doesn't look real. not worth looking at under magnification! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[1.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# see example\n",
    "print(tokenizer.decode(encoded_dataset[\"train\"][0][\"input_ids\"]))\n",
    "print(encoded_dataset[\"train\"][0][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset a standard torch dataset by converting to tensors (and more?)\n",
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)#.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add Warmup?\n",
    "Experiment with learning rate\n",
    "experiment with batch size\n",
    "experient with gradient_accumulation_steps\n",
    "another metric?\n",
    "Choose another optimizer: RMSprop, SGD...\n",
    "Increase the learning rate by default and then use the callback ReduceLROnPlateau\n",
    "\"\"\"\n",
    "# use keras instead of huggung face to make it easier to work with messing with layers\n",
    "# remove entries greater than 512 words to remove noise\n",
    "# enchance data by repeatung key terms\n",
    "# cut 512 from middle of the dataset\n",
    "# try giving it only the labels with 5 word context\n",
    "# try doing subtext technique to give it 1000 words\n",
    "\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 8\n",
    "lr = 2e-5\n",
    "metric_name = \"f1\"\n",
    "decay = 0.01\n",
    "\n",
    "model_path = \"/mnt/d/Skull/coding/model/\"\n",
    "tokenizer_path = \"./tokenizer\"\n",
    "logs_path = \"./logs\"\n",
    "\n",
    "with open('../../../hg_token.txt', 'r') as file:\n",
    "\thg_token = file.read()\n",
    "\n",
    "\n",
    "# args for training the model\n",
    "# save the model every epoch and choose the best performing epoch as the final version of the model\n",
    "args = TrainingArguments(\n",
    "\teval_strategy = \"epoch\",\n",
    "\tsave_strategy = \"epoch\",\n",
    "    # save_total_limit = 5,\n",
    "\tlogging_strategy = \"epoch\",\n",
    "\tlearning_rate = lr,\n",
    "\tper_device_train_batch_size = batch_size,\n",
    "\tper_device_eval_batch_size = batch_size,\n",
    "\tnum_train_epochs = num_epochs,\n",
    "\tweight_decay = decay,\n",
    "\tload_best_model_at_end = True,\n",
    "\tmetric_for_best_model = metric_name,\n",
    "\tlogging_dir = logs_path,\n",
    "\toutput_dir = model_path,\n",
    "    warmup_steps=100,\n",
    "\t# use_mps_device = True,\n",
    "\tuse_cpu = False,\n",
    "\tlogging_steps = 1,\n",
    "\t# gradient_accumulation_steps=2,\n",
    "\thub_token = hg_token,\n",
    "\thub_model_id = \"raasikhk/carlos_bert_v2\",\n",
    "\tpush_to_hub=True,\n",
    ")\n",
    "\n",
    "import os\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, confusion_matrix\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "from numpy import ndarray\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "\n",
    "def get_next_image_number(directory: str) -> int:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        return 1\n",
    "    images = glob.glob(os.path.join(directory, '*.png'))\n",
    "    if not images:\n",
    "        return 1\n",
    "    numbers = [int(os.path.basename(image).split('_')[0]) for image in images]\n",
    "    return max(numbers) + 1\n",
    "\n",
    "def plot_confusion_matrix(cm, save_path, title):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(cm))\n",
    "    plt.xticks(tick_marks, tick_marks, rotation=45)\n",
    "    plt.yticks(tick_marks, tick_marks)\n",
    "    \n",
    "    for i in range(len(cm)):\n",
    "        for j in range(len(cm[i])):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > cm.max() / 2. else \"black\")\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def my_accuracy_score(y_true: np.ndarray, y_pred: np.ndarray) -> Tuple[int, int, int, int]:\n",
    "    directory = 'cm'\n",
    "    image_number = get_next_image_number(directory)\n",
    "    \n",
    "    labels = [\"Subjective\", \"Gender\", \"Jargon\", \"Social\"]\n",
    "    true_pos_list, false_pos_list, true_neg_list, false_neg_list = [], [], [], []\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        save_path = os.path.join(directory, f'{image_number}_{label}.png')\n",
    "        \n",
    "        # Calculate confusion matrix for the current label\n",
    "        cm = confusion_matrix(y_true[:, i], y_pred[:, i])\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plot_confusion_matrix(cm, save_path, f'Confusion Matrix - {label}')\n",
    "        \n",
    "        # Calculate true positives, false positives, true negatives, false negatives\n",
    "        true_pos = np.sum((y_true[:, i] == 1) & (y_pred[:, i] == 1))\n",
    "        false_pos = np.sum((y_true[:, i] == 0) & (y_pred[:, i] == 1))\n",
    "        true_neg = np.sum((y_true[:, i] == 0) & (y_pred[:, i] == 0))\n",
    "        false_neg = np.sum((y_true[:, i] == 1) & (y_pred[:, i] == 0))\n",
    "        \n",
    "        true_pos_list.append(true_pos)\n",
    "        false_pos_list.append(false_pos)\n",
    "        true_neg_list.append(true_neg)\n",
    "        false_neg_list.append(false_neg)\n",
    "    \n",
    "    return (\n",
    "        sum(true_pos_list), sum(false_pos_list), \n",
    "        sum(true_neg_list), sum(false_neg_list)\n",
    "    )\n",
    "\n",
    "\n",
    "def partial_accuracy_score(y_true: ndarray, y_pred: ndarray):\n",
    "\tnum_objects = len(y_true)\n",
    "\tnum_labels = len(y_true)*4\n",
    "\tcorrect_predictions = 0\n",
    "\t\n",
    "\tfor i in range(num_objects):\n",
    "\t\tfor j in range(len(y_true[i])):\n",
    "\t\t\tif y_true[i][j] == y_pred[i][j]:\n",
    "\t\t\t\tcorrect_predictions += 1\n",
    "\t\n",
    "\taccuracy = correct_predictions / num_labels\n",
    "\treturn accuracy\n",
    "\n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "\t# first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "\tsigmoid = torch.nn.Sigmoid()\n",
    "\tprobs = sigmoid(torch.Tensor(predictions))\n",
    "\t# next, use threshold to turn them into integer predictions\n",
    "\ty_pred = np.zeros(probs.shape)\n",
    "\ty_pred[np.where(probs >= threshold)] = 1\n",
    "\t# finally, compute metrics\n",
    "\ty_true = labels\n",
    "\tf1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "\troc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "\taccuracy = accuracy_score(y_true, y_pred)\n",
    "\tmyacc = partial_accuracy_score(y_true, y_pred)\n",
    "\ttrue_pos, false_pos, true_neg, false_neg = my_accuracy_score(y_true, y_pred)\n",
    "\t# return as dictionary\n",
    "\tmetrics = {'f1': f1_micro_average,\n",
    "\t\t\t\t'roc_auc': roc_auc,\n",
    "\t\t\t\t'exact_match_acc': accuracy,\n",
    "\t\t\t\t\"partial_acc\": myacc,\n",
    "\t\t\t\t'true_pos': true_pos,\n",
    "        \t\t'true_neg': true_neg,\n",
    "        \t\t'false_neg': false_neg,\n",
    "\t\t\t\t'false_pos': false_pos}\n",
    "\treturn metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "\tpreds = p.predictions[0] if isinstance(p.predictions, \n",
    "\t\t\ttuple) else p.predictions\n",
    "\tresult = multi_label_metrics(\n",
    "\t\tpredictions=preds, \n",
    "\t\tlabels=p.label_ids)\n",
    "\treturn result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "\tmodel,\n",
    "\targs,\n",
    "\ttrain_dataset=encoded_dataset[\"train\"],\n",
    "\teval_dataset=encoded_dataset[\"validation\"],\n",
    "\ttokenizer=tokenizer,\n",
    "\tcompute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16000' max='16000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16000/16000 2:42:51, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Exact Match Acc</th>\n",
       "      <th>Partial Acc</th>\n",
       "      <th>True Pos</th>\n",
       "      <th>True Neg</th>\n",
       "      <th>False Neg</th>\n",
       "      <th>False Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.428400</td>\n",
       "      <td>0.301900</td>\n",
       "      <td>0.577540</td>\n",
       "      <td>0.737861</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.877329</td>\n",
       "      <td>54</td>\n",
       "      <td>511</td>\n",
       "      <td>47</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.283439</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.705277</td>\n",
       "      <td>0.596273</td>\n",
       "      <td>0.883540</td>\n",
       "      <td>45</td>\n",
       "      <td>524</td>\n",
       "      <td>56</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.262400</td>\n",
       "      <td>0.294516</td>\n",
       "      <td>0.602041</td>\n",
       "      <td>0.758930</td>\n",
       "      <td>0.577640</td>\n",
       "      <td>0.878882</td>\n",
       "      <td>59</td>\n",
       "      <td>507</td>\n",
       "      <td>42</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>0.320963</td>\n",
       "      <td>0.528090</td>\n",
       "      <td>0.705049</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>47</td>\n",
       "      <td>513</td>\n",
       "      <td>54</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.346100</td>\n",
       "      <td>0.547264</td>\n",
       "      <td>0.730841</td>\n",
       "      <td>0.515528</td>\n",
       "      <td>0.858696</td>\n",
       "      <td>55</td>\n",
       "      <td>498</td>\n",
       "      <td>46</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.153400</td>\n",
       "      <td>0.371906</td>\n",
       "      <td>0.554348</td>\n",
       "      <td>0.723009</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.872671</td>\n",
       "      <td>51</td>\n",
       "      <td>511</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.120500</td>\n",
       "      <td>0.389813</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.736366</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>55</td>\n",
       "      <td>504</td>\n",
       "      <td>46</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.101000</td>\n",
       "      <td>0.425688</td>\n",
       "      <td>0.582915</td>\n",
       "      <td>0.750296</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.871118</td>\n",
       "      <td>58</td>\n",
       "      <td>503</td>\n",
       "      <td>43</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.076100</td>\n",
       "      <td>0.438775</td>\n",
       "      <td>0.579186</td>\n",
       "      <td>0.765266</td>\n",
       "      <td>0.527950</td>\n",
       "      <td>0.855590</td>\n",
       "      <td>64</td>\n",
       "      <td>487</td>\n",
       "      <td>37</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.067000</td>\n",
       "      <td>0.442058</td>\n",
       "      <td>0.581281</td>\n",
       "      <td>0.752484</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>59</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>0.491115</td>\n",
       "      <td>0.554974</td>\n",
       "      <td>0.728306</td>\n",
       "      <td>0.540373</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>53</td>\n",
       "      <td>506</td>\n",
       "      <td>48</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.043700</td>\n",
       "      <td>0.517753</td>\n",
       "      <td>0.604444</td>\n",
       "      <td>0.785068</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.861801</td>\n",
       "      <td>68</td>\n",
       "      <td>487</td>\n",
       "      <td>33</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.036700</td>\n",
       "      <td>0.553837</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>0.768375</td>\n",
       "      <td>0.509317</td>\n",
       "      <td>0.854037</td>\n",
       "      <td>65</td>\n",
       "      <td>485</td>\n",
       "      <td>36</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.029600</td>\n",
       "      <td>0.562667</td>\n",
       "      <td>0.606635</td>\n",
       "      <td>0.774474</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.871118</td>\n",
       "      <td>64</td>\n",
       "      <td>497</td>\n",
       "      <td>37</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.537216</td>\n",
       "      <td>0.603774</td>\n",
       "      <td>0.773554</td>\n",
       "      <td>0.540373</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>64</td>\n",
       "      <td>496</td>\n",
       "      <td>37</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.023400</td>\n",
       "      <td>0.602558</td>\n",
       "      <td>0.574163</td>\n",
       "      <td>0.752831</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.861801</td>\n",
       "      <td>60</td>\n",
       "      <td>495</td>\n",
       "      <td>41</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.021300</td>\n",
       "      <td>0.584517</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.745692</td>\n",
       "      <td>0.527950</td>\n",
       "      <td>0.863354</td>\n",
       "      <td>58</td>\n",
       "      <td>498</td>\n",
       "      <td>43</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>0.575784</td>\n",
       "      <td>0.593301</td>\n",
       "      <td>0.764573</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>62</td>\n",
       "      <td>497</td>\n",
       "      <td>39</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>0.580219</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.785871</td>\n",
       "      <td>0.590062</td>\n",
       "      <td>0.883540</td>\n",
       "      <td>65</td>\n",
       "      <td>504</td>\n",
       "      <td>36</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.662526</td>\n",
       "      <td>0.561224</td>\n",
       "      <td>0.735445</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.866460</td>\n",
       "      <td>55</td>\n",
       "      <td>503</td>\n",
       "      <td>46</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>0.660996</td>\n",
       "      <td>0.595349</td>\n",
       "      <td>0.770791</td>\n",
       "      <td>0.534161</td>\n",
       "      <td>0.864907</td>\n",
       "      <td>64</td>\n",
       "      <td>493</td>\n",
       "      <td>37</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>0.666699</td>\n",
       "      <td>0.561224</td>\n",
       "      <td>0.735445</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.866460</td>\n",
       "      <td>55</td>\n",
       "      <td>503</td>\n",
       "      <td>46</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>0.685525</td>\n",
       "      <td>0.556098</td>\n",
       "      <td>0.738900</td>\n",
       "      <td>0.527950</td>\n",
       "      <td>0.858696</td>\n",
       "      <td>57</td>\n",
       "      <td>496</td>\n",
       "      <td>44</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.675748</td>\n",
       "      <td>0.595349</td>\n",
       "      <td>0.770791</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.864907</td>\n",
       "      <td>64</td>\n",
       "      <td>493</td>\n",
       "      <td>37</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.695459</td>\n",
       "      <td>0.598131</td>\n",
       "      <td>0.771712</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.866460</td>\n",
       "      <td>64</td>\n",
       "      <td>494</td>\n",
       "      <td>37</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>0.737124</td>\n",
       "      <td>0.566210</td>\n",
       "      <td>0.755365</td>\n",
       "      <td>0.509317</td>\n",
       "      <td>0.852484</td>\n",
       "      <td>62</td>\n",
       "      <td>487</td>\n",
       "      <td>39</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.715960</td>\n",
       "      <td>0.596330</td>\n",
       "      <td>0.773900</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.863354</td>\n",
       "      <td>65</td>\n",
       "      <td>491</td>\n",
       "      <td>36</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>0.731244</td>\n",
       "      <td>0.579439</td>\n",
       "      <td>0.759969</td>\n",
       "      <td>0.540373</td>\n",
       "      <td>0.860248</td>\n",
       "      <td>62</td>\n",
       "      <td>492</td>\n",
       "      <td>39</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.740838</td>\n",
       "      <td>0.582524</td>\n",
       "      <td>0.755593</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.866460</td>\n",
       "      <td>60</td>\n",
       "      <td>498</td>\n",
       "      <td>41</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>0.737485</td>\n",
       "      <td>0.566038</td>\n",
       "      <td>0.750068</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>60</td>\n",
       "      <td>492</td>\n",
       "      <td>41</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.011200</td>\n",
       "      <td>0.768076</td>\n",
       "      <td>0.575472</td>\n",
       "      <td>0.755940</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.860248</td>\n",
       "      <td>61</td>\n",
       "      <td>493</td>\n",
       "      <td>40</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>0.764400</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.751910</td>\n",
       "      <td>0.534161</td>\n",
       "      <td>0.860248</td>\n",
       "      <td>60</td>\n",
       "      <td>494</td>\n",
       "      <td>41</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>0.761523</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.757435</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>60</td>\n",
       "      <td>500</td>\n",
       "      <td>41</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.782515</td>\n",
       "      <td>0.588785</td>\n",
       "      <td>0.765841</td>\n",
       "      <td>0.534161</td>\n",
       "      <td>0.863354</td>\n",
       "      <td>63</td>\n",
       "      <td>493</td>\n",
       "      <td>38</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.799107</td>\n",
       "      <td>0.568720</td>\n",
       "      <td>0.750989</td>\n",
       "      <td>0.527950</td>\n",
       "      <td>0.858696</td>\n",
       "      <td>60</td>\n",
       "      <td>493</td>\n",
       "      <td>41</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.814241</td>\n",
       "      <td>0.586538</td>\n",
       "      <td>0.759623</td>\n",
       "      <td>0.540373</td>\n",
       "      <td>0.866460</td>\n",
       "      <td>61</td>\n",
       "      <td>497</td>\n",
       "      <td>40</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.825904</td>\n",
       "      <td>0.527919</td>\n",
       "      <td>0.716910</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.855590</td>\n",
       "      <td>52</td>\n",
       "      <td>499</td>\n",
       "      <td>49</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.790843</td>\n",
       "      <td>0.563380</td>\n",
       "      <td>0.749148</td>\n",
       "      <td>0.515528</td>\n",
       "      <td>0.855590</td>\n",
       "      <td>60</td>\n",
       "      <td>491</td>\n",
       "      <td>41</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>0.780152</td>\n",
       "      <td>0.597156</td>\n",
       "      <td>0.768603</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>63</td>\n",
       "      <td>496</td>\n",
       "      <td>38</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>0.819879</td>\n",
       "      <td>0.520408</td>\n",
       "      <td>0.711960</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.854037</td>\n",
       "      <td>51</td>\n",
       "      <td>499</td>\n",
       "      <td>50</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>0.824659</td>\n",
       "      <td>0.557078</td>\n",
       "      <td>0.749494</td>\n",
       "      <td>0.509317</td>\n",
       "      <td>0.849379</td>\n",
       "      <td>61</td>\n",
       "      <td>486</td>\n",
       "      <td>40</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.780107</td>\n",
       "      <td>0.579710</td>\n",
       "      <td>0.754672</td>\n",
       "      <td>0.534161</td>\n",
       "      <td>0.864907</td>\n",
       "      <td>60</td>\n",
       "      <td>497</td>\n",
       "      <td>41</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.799289</td>\n",
       "      <td>0.590476</td>\n",
       "      <td>0.763653</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.866460</td>\n",
       "      <td>62</td>\n",
       "      <td>496</td>\n",
       "      <td>39</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.811441</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.743504</td>\n",
       "      <td>0.540373</td>\n",
       "      <td>0.866460</td>\n",
       "      <td>57</td>\n",
       "      <td>501</td>\n",
       "      <td>44</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>0.816334</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.737633</td>\n",
       "      <td>0.540373</td>\n",
       "      <td>0.863354</td>\n",
       "      <td>56</td>\n",
       "      <td>500</td>\n",
       "      <td>45</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.796457</td>\n",
       "      <td>0.600939</td>\n",
       "      <td>0.772633</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>64</td>\n",
       "      <td>495</td>\n",
       "      <td>37</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>0.808027</td>\n",
       "      <td>0.591133</td>\n",
       "      <td>0.758356</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.871118</td>\n",
       "      <td>60</td>\n",
       "      <td>501</td>\n",
       "      <td>41</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>0.812888</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.756514</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>60</td>\n",
       "      <td>499</td>\n",
       "      <td>41</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.806781</td>\n",
       "      <td>0.574163</td>\n",
       "      <td>0.752831</td>\n",
       "      <td>0.527950</td>\n",
       "      <td>0.861801</td>\n",
       "      <td>60</td>\n",
       "      <td>495</td>\n",
       "      <td>41</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>0.760150</td>\n",
       "      <td>0.597015</td>\n",
       "      <td>0.760197</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.874224</td>\n",
       "      <td>60</td>\n",
       "      <td>503</td>\n",
       "      <td>41</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.797031</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.757435</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>60</td>\n",
       "      <td>500</td>\n",
       "      <td>41</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.772251</td>\n",
       "      <td>0.619469</td>\n",
       "      <td>0.795890</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.866460</td>\n",
       "      <td>70</td>\n",
       "      <td>488</td>\n",
       "      <td>31</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.810753</td>\n",
       "      <td>0.583732</td>\n",
       "      <td>0.758702</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.864907</td>\n",
       "      <td>61</td>\n",
       "      <td>496</td>\n",
       "      <td>40</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.806970</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.756514</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>60</td>\n",
       "      <td>499</td>\n",
       "      <td>41</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.843356</td>\n",
       "      <td>0.579710</td>\n",
       "      <td>0.754672</td>\n",
       "      <td>0.540373</td>\n",
       "      <td>0.864907</td>\n",
       "      <td>60</td>\n",
       "      <td>497</td>\n",
       "      <td>41</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>0.843511</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.755247</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.872671</td>\n",
       "      <td>59</td>\n",
       "      <td>503</td>\n",
       "      <td>42</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.893957</td>\n",
       "      <td>0.593607</td>\n",
       "      <td>0.772979</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.861801</td>\n",
       "      <td>65</td>\n",
       "      <td>490</td>\n",
       "      <td>36</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.837287</td>\n",
       "      <td>0.582011</td>\n",
       "      <td>0.741890</td>\n",
       "      <td>0.577640</td>\n",
       "      <td>0.877329</td>\n",
       "      <td>55</td>\n",
       "      <td>510</td>\n",
       "      <td>46</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.857965</td>\n",
       "      <td>0.566038</td>\n",
       "      <td>0.750068</td>\n",
       "      <td>0.534161</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>60</td>\n",
       "      <td>492</td>\n",
       "      <td>41</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.857035</td>\n",
       "      <td>0.594059</td>\n",
       "      <td>0.759276</td>\n",
       "      <td>0.583851</td>\n",
       "      <td>0.872671</td>\n",
       "      <td>60</td>\n",
       "      <td>502</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.866605</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.746613</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.864907</td>\n",
       "      <td>58</td>\n",
       "      <td>499</td>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.865623</td>\n",
       "      <td>0.575916</td>\n",
       "      <td>0.740049</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.874224</td>\n",
       "      <td>55</td>\n",
       "      <td>508</td>\n",
       "      <td>46</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.863763</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.741316</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>56</td>\n",
       "      <td>504</td>\n",
       "      <td>45</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.859152</td>\n",
       "      <td>0.575610</td>\n",
       "      <td>0.750643</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.864907</td>\n",
       "      <td>59</td>\n",
       "      <td>498</td>\n",
       "      <td>42</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.851033</td>\n",
       "      <td>0.575916</td>\n",
       "      <td>0.740049</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.874224</td>\n",
       "      <td>55</td>\n",
       "      <td>508</td>\n",
       "      <td>46</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.857434</td>\n",
       "      <td>0.574359</td>\n",
       "      <td>0.742237</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.871118</td>\n",
       "      <td>56</td>\n",
       "      <td>505</td>\n",
       "      <td>45</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.865957</td>\n",
       "      <td>0.579710</td>\n",
       "      <td>0.754672</td>\n",
       "      <td>0.540373</td>\n",
       "      <td>0.864907</td>\n",
       "      <td>60</td>\n",
       "      <td>497</td>\n",
       "      <td>41</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.861186</td>\n",
       "      <td>0.592965</td>\n",
       "      <td>0.756168</td>\n",
       "      <td>0.577640</td>\n",
       "      <td>0.874224</td>\n",
       "      <td>59</td>\n",
       "      <td>504</td>\n",
       "      <td>42</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.864054</td>\n",
       "      <td>0.584158</td>\n",
       "      <td>0.753405</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>59</td>\n",
       "      <td>501</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>0.868980</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.745346</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>57</td>\n",
       "      <td>503</td>\n",
       "      <td>44</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.870294</td>\n",
       "      <td>0.578680</td>\n",
       "      <td>0.746267</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.871118</td>\n",
       "      <td>57</td>\n",
       "      <td>504</td>\n",
       "      <td>44</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.868369</td>\n",
       "      <td>0.572864</td>\n",
       "      <td>0.744425</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>57</td>\n",
       "      <td>502</td>\n",
       "      <td>44</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>0.871072</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.745346</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>57</td>\n",
       "      <td>503</td>\n",
       "      <td>44</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.872705</td>\n",
       "      <td>0.574257</td>\n",
       "      <td>0.747534</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.866460</td>\n",
       "      <td>58</td>\n",
       "      <td>500</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.869562</td>\n",
       "      <td>0.582915</td>\n",
       "      <td>0.750296</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.871118</td>\n",
       "      <td>58</td>\n",
       "      <td>503</td>\n",
       "      <td>43</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.876670</td>\n",
       "      <td>0.582915</td>\n",
       "      <td>0.750296</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.871118</td>\n",
       "      <td>58</td>\n",
       "      <td>503</td>\n",
       "      <td>43</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.877828</td>\n",
       "      <td>0.581281</td>\n",
       "      <td>0.752484</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>59</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.895867</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.756514</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>60</td>\n",
       "      <td>499</td>\n",
       "      <td>41</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.884201</td>\n",
       "      <td>0.578431</td>\n",
       "      <td>0.751564</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.866460</td>\n",
       "      <td>59</td>\n",
       "      <td>499</td>\n",
       "      <td>42</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.886932</td>\n",
       "      <td>0.581281</td>\n",
       "      <td>0.752484</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>59</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.885420</td>\n",
       "      <td>0.581281</td>\n",
       "      <td>0.752484</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>59</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.885376</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.746613</td>\n",
       "      <td>0.540373</td>\n",
       "      <td>0.864907</td>\n",
       "      <td>58</td>\n",
       "      <td>499</td>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.885253</td>\n",
       "      <td>0.578680</td>\n",
       "      <td>0.746267</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.871118</td>\n",
       "      <td>57</td>\n",
       "      <td>504</td>\n",
       "      <td>44</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.891949</td>\n",
       "      <td>0.562814</td>\n",
       "      <td>0.738554</td>\n",
       "      <td>0.540373</td>\n",
       "      <td>0.864907</td>\n",
       "      <td>56</td>\n",
       "      <td>501</td>\n",
       "      <td>45</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.889668</td>\n",
       "      <td>0.577114</td>\n",
       "      <td>0.748455</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>58</td>\n",
       "      <td>501</td>\n",
       "      <td>43</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.890481</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.746613</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.864907</td>\n",
       "      <td>58</td>\n",
       "      <td>499</td>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.890098</td>\n",
       "      <td>0.581281</td>\n",
       "      <td>0.752484</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>59</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.888327</td>\n",
       "      <td>0.582915</td>\n",
       "      <td>0.750296</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.871118</td>\n",
       "      <td>58</td>\n",
       "      <td>503</td>\n",
       "      <td>43</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.883289</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.757435</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>60</td>\n",
       "      <td>500</td>\n",
       "      <td>41</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.885491</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.743504</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.866460</td>\n",
       "      <td>57</td>\n",
       "      <td>501</td>\n",
       "      <td>44</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.889501</td>\n",
       "      <td>0.574257</td>\n",
       "      <td>0.747534</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.866460</td>\n",
       "      <td>58</td>\n",
       "      <td>500</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.581281</td>\n",
       "      <td>0.752484</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>59</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.896721</td>\n",
       "      <td>0.591133</td>\n",
       "      <td>0.758356</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.871118</td>\n",
       "      <td>60</td>\n",
       "      <td>501</td>\n",
       "      <td>41</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.896230</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.757435</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>60</td>\n",
       "      <td>500</td>\n",
       "      <td>41</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.897151</td>\n",
       "      <td>0.577114</td>\n",
       "      <td>0.748455</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>58</td>\n",
       "      <td>501</td>\n",
       "      <td>43</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.897657</td>\n",
       "      <td>0.577114</td>\n",
       "      <td>0.748455</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>58</td>\n",
       "      <td>501</td>\n",
       "      <td>43</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.897338</td>\n",
       "      <td>0.577114</td>\n",
       "      <td>0.748455</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>58</td>\n",
       "      <td>501</td>\n",
       "      <td>43</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.897559</td>\n",
       "      <td>0.577114</td>\n",
       "      <td>0.748455</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>58</td>\n",
       "      <td>501</td>\n",
       "      <td>43</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.897187</td>\n",
       "      <td>0.577114</td>\n",
       "      <td>0.748455</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>58</td>\n",
       "      <td>501</td>\n",
       "      <td>43</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.897613</td>\n",
       "      <td>0.577114</td>\n",
       "      <td>0.748455</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>58</td>\n",
       "      <td>501</td>\n",
       "      <td>43</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16000, training_loss=0.028679579094052315, metrics={'train_runtime': 9773.3715, 'train_samples_per_second': 13.097, 'train_steps_per_second': 1.637, 'total_flos': 3.3678819852288e+16, 'train_loss': 0.028679579094052315, 'epoch': 100.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TrainOutput(global_step=1200, training_loss=0.05973712073231582, metrics={'train_runtime': 1398.0176, 'train_samples_per_second': 13.734, 'train_steps_per_second': 0.858, 'total_flos': 5051822977843200.0, 'train_loss': 0.05973712073231582, 'epoch': 15.0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5802189707756042,\n",
       " 'eval_f1': 0.6341463414634146,\n",
       " 'eval_roc_auc': 0.7858705760078771,\n",
       " 'eval_exact_match_acc': 0.5900621118012422,\n",
       " 'eval_partial_acc': 0.8835403726708074,\n",
       " 'eval_true_pos': 65,\n",
       " 'eval_true_neg': 504,\n",
       " 'eval_false_neg': 36,\n",
       " 'eval_false_pos': 39,\n",
       " 'eval_runtime': 3.6007,\n",
       " 'eval_samples_per_second': 44.714,\n",
       " 'eval_steps_per_second': 5.832,\n",
       " 'epoch': 100.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'eval_loss': 0.324933260679245,\n",
    " 'eval_f1': 0.7070707070707071,\n",
    " 'eval_roc_auc': 0.8216727750123078,\n",
    " 'eval_accuracy': 0.7018633540372671,\n",
    " 'eval_runtime': 3.7853,\n",
    " 'eval_samples_per_second': 42.533,\n",
    " 'eval_steps_per_second': 2.906,\n",
    " 'epoch': 15.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard\n",
      "  Using cached tensorboard-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Downloading grpcio-1.65.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Using cached Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/raasikhk/Coding/ai.xperience/carlos-artifact-tagging-bias/myenv/lib/python3.11/site-packages (from tensorboard) (1.26.4)\n",
      "Collecting protobuf!=4.24.0,<5.0.0,>=3.19.6 (from tensorboard)\n",
      "  Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/raasikhk/Coding/ai.xperience/carlos-artifact-tagging-bias/myenv/lib/python3.11/site-packages (from tensorboard) (65.5.0)\n",
      "Requirement already satisfied: six>1.9 in /home/raasikhk/Coding/ai.xperience/carlos-artifact-tagging-bias/myenv/lib/python3.11/site-packages (from tensorboard) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Using cached werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/raasikhk/Coding/ai.xperience/carlos-artifact-tagging-bias/myenv/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "Using cached tensorboard-2.17.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading grpcio-1.65.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Using cached werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "Installing collected packages: werkzeug, tensorboard-data-server, protobuf, markdown, grpcio, absl-py, tensorboard\n",
      "Successfully installed absl-py-2.1.0 grpcio-1.65.1 markdown-3.6 protobuf-4.25.3 tensorboard-2.17.0 tensorboard-data-server-0.7.2 werkzeug-3.0.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "TensorFlow installation not found - running with reduced feature set.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721830932.526459   75105 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache\n",
      "I0000 00:00:1721830932.529415   75163 subchannel.cc:806] subchannel 0x5581862e6a50 {address=ipv6:%5B::1%5D:35793, args={grpc.client_channel_factory=0x5581862a1bb0, grpc.default_authority=localhost:35793, grpc.internal.channel_credentials=0x5581862f07a0, grpc.internal.client_channel_call_destination=0x7f83373a73d0, grpc.internal.event_engine=0x5581860a2260, grpc.internal.security_connector=0x558185ef7770, grpc.internal.subchannel_pool=0x558185d6a220, grpc.max_receive_message_length=268435456, grpc.primary_user_agent=grpc-python/1.65.1, grpc.resource_quota=0x558185a85cd0, grpc.server_uri=dns:///localhost:35793}}: connect failed (UNKNOWN:Failed to connect to remote host: connect: Connection refused (111) {created_time:\"2024-07-24T10:22:12.529126573-04:00\"}), backing off for 1000 ms\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.17.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# view logs (only needed for analysis)\n",
    "# !pip install tensorboard\n",
    "!tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"model\")\n",
    "# tokenizer.save_pretrained(\"tokenizer\")\n",
    "# tokenizer = transformers.BertTokenizer.from_pretrained(\"tokenizer\")\n",
    "# model = transformers.BertForSequenceClassification.from_pretrained(\"model/checkpoint-1200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0007, 0.0031, 0.0750, 0.0016], grad_fn=<SigmoidBackward0>)\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# test a description\n",
    "text = \"December 1992 lead-in term added. January 1991 alternate term added. Object fumigated in Orkin's Piedmont vault with Vikane in 1994\"\n",
    "\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
    "\n",
    "outputs = trainer.model(**encoding)\n",
    "\n",
    "logits = outputs.logits\n",
    "logits.shape\n",
    "\n",
    "# apply sigmoid + threshold\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(logits.squeeze().cpu())\n",
    "print(probs)\n",
    "predictions = np.zeros(probs.shape)\n",
    "predictions[np.where(probs >= 0.5)] = 1\n",
    "# turn predicted id's into actual label names\n",
    "predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREDICTION**\n",
    "\n",
    "Run the first code block only if you have the model folder and have NOT done training above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The artifact was created in the 19th century and is considered highly valuable.\n",
      "Predictions: [{'label': 'Subjective', 'score': 0.09203276038169861}, {'label': 'Jargon', 'score': 0.026056140661239624}, {'label': 'Social', 'score': 0.009623771533370018}, {'label': 'Gender', 'score': 0.00038654549280181527}]\n",
      "\n",
      "Text: This piece shows signs of heavy wear and might not be authentic.\n",
      "Predictions: [{'label': 'Subjective', 'score': 0.8277921676635742}, {'label': 'Jargon', 'score': 0.043133534491062164}, {'label': 'Social', 'score': 0.0035685293842107058}, {'label': 'Gender', 'score': 0.0008079292019829154}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST THE MODEL\n",
    "# IF YOU DONT WANT TO TRAIN LOAD MODEL FROM HUGGINGFACE\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "import torch\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "if tokenizer == None:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"raasikhk/carlos_bert_v1\")\n",
    "if model == None:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"raasikhk/carlos_bert_v1\")\n",
    "\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, top_k=None, truncation=True, padding=True, device=device)\n",
    "\n",
    "\n",
    "texts = [\n",
    "    \"The artifact was created in the 19th century and is considered highly valuable.\",\n",
    "    \"This piece shows signs of heavy wear and might not be authentic.\"\n",
    "]\n",
    "\n",
    "# Make predictions\n",
    "predictions = pipe(texts)\n",
    "\n",
    "# Print the predictions\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predictions: {predictions[i]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Partial correct score: \n",
      "551 / 644\n",
      "0.8555900621118012\n",
      "\n",
      "All correct score: \n",
      "272 / 644\n",
      "0.422360248447205\n"
     ]
    }
   ],
   "source": [
    "# CALCULATE ACCURACY\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "import torch\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# if tokenizer == None:\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"raasikhk/carlos_bert_v1\")\n",
    "# if model == None:\n",
    "#     model = AutoModelForSequenceClassification.from_pretrained(\"raasikhk/carlos_bert_v1\")\n",
    "\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, top_k=None, truncation=True, padding=True, device=device)\n",
    "\n",
    "# Replace with your test dataset\n",
    "test_descriptions = dataset[\"validation\"][\"Description\"]\n",
    "\n",
    "# Get predictions\n",
    "predictions = pipe(test_descriptions)\n",
    "\n",
    "labels= [\"Subjective\", \"Gender\", \"Jargon\", \"Social\"]\n",
    "\n",
    "print(\"\\nPartial correct score: \")\n",
    "\n",
    "score = 0\n",
    "total = 0\n",
    "for p in predictions:\n",
    "\n",
    "    for i in range(4):\n",
    "        if p[i][\"score\"] >=0.5:\n",
    "            prediction=1\n",
    "        else:\n",
    "            prediction=0\n",
    "        \n",
    "        if dataset[\"validation\"][i][labels[i]] == prediction:\n",
    "            score +=1\n",
    "        total+=1\n",
    "print(f\"{score} / {total}\")\n",
    "print(score/total)\n",
    "\n",
    "print(\"\\nAll correct score: \")\n",
    "\n",
    "score = 0\n",
    "total = 0\n",
    "for p in predictions:\n",
    "    all_c=True\n",
    "    for i in range(4):\n",
    "        if p[i][\"score\"] >=0.5:\n",
    "            prediction=1\n",
    "        else:\n",
    "            prediction=0\n",
    "        \n",
    "        if dataset[\"validation\"][i][labels[i]] != prediction:\n",
    "            all_c = False\n",
    "        total+=1\n",
    "        if (all_c):\n",
    "            score+=1\n",
    "\n",
    "print(f\"{score} / {total}\")\n",
    "print(score/total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
