# The basic idea of this program is to perform frequency analysis on the text entries in the data obtained from the original cleaning process.
# For the top 300 words with the highest frequency, we will perform sentiment analysis. Based on their scores in subjectivity and sentiment, we will determine whether they are biased or not.

import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from collections import Counter
import re
from textblob import TextBlob

# Download necessary NLTK resources
nltk.download('stopwords')
nltk.download('punkt')

# Read the Excel file (Data obtained from sentiment analysis)
file_path = 'C:/Users/LC180/Desktop/AI.XPERIENCE/Cleaned_data1.xlsx'
targetFile = pd.read_excel(file_path)

# Display the first few rows of the dataframe to confirm it has been read correctly
print(targetFile.head())

# Extract the target column and convert all entries to strings, handling NaN values
text_data = targetFile['TextEntry'].astype(str).fillna('')

# Combine all text entries into a single string
combined_text = " ".join(text_data)

# Get the stop words from NLTK
stop_words = set(stopwords.words('english'))

# Function to preprocess text using the NLTK stop words list
def preprocess_text_nltk(text):
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text)
    words = word_tokenize(text)
    filtered_words = [word for word in words if word not in stop_words]
    return filtered_words

# Apply the preprocessing function to the text entries
targetFile['Processed_Text'] = targetFile['TextEntry'].astype(str).fillna('').apply(preprocess_text_nltk)

# Display the first few rows of the dataframe to confirm preprocessing
print(targetFile[['TextEntry', 'Processed_Text']].head())

# Flatten the list of tokens and calculate word frequencies
all_tokens = [token for sublist in targetFile['Processed_Text'] for token in sublist]
word_freq = Counter(all_tokens)

# Display the most common words
most_common_words = word_freq.most_common(300)  # Get the top 300 common words
print(most_common_words)

# Calculate sentiment scores using TextBlob for the top 300 words
def get_sentiment(text):
    blob = TextBlob(text)
    return blob.sentiment.polarity, blob.sentiment.subjectivity

# Split text data into sentences
sentences = [sentence for text in text_data for sentence in sent_tokenize(text)]

# Calculate sentiment for each sentence once
sentence_sentiments = {}
for sentence in sentences:
    sentiment, subjectivity = get_sentiment(sentence)
    sentence_sentiments[sentence] = (sentiment, subjectivity)

# Aggregate sentiment and subjectivity for each word in the top 300 words
word_sentiment_subjectivity = []
for word, freq in most_common_words:
    sentiments = []
    subjectivities = []
    for sentence, (sentiment, subjectivity) in sentence_sentiments.items():
        if word in sentence:
            sentiments.append(sentiment)
            subjectivities.append(subjectivity)
    avg_sentiment = sum(sentiments) / len(sentiments) if sentiments else 0
    avg_subjectivity = sum(subjectivities) / len(subjectivities) if subjectivities else 0
    word_sentiment_subjectivity.append((word, avg_sentiment, avg_subjectivity, freq))

# Convert to DataFrame for better visualization
sentiment_subjectivity_df = pd.DataFrame(word_sentiment_subjectivity, columns=['Word', 'Avg_Sentiment', 'Avg_Subjectivity', 'Frequency'])
print(sentiment_subjectivity_df)

# Save the DataFrame to an Excel file
output_file_path = 'C:/Users/LC180/Desktop/AI.XPERIENCE/BiasedWord2.xlsx'
sentiment_subjectivity_df.to_excel(output_file_path, index=False)
