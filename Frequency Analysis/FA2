# The basic idea of this program is to perform frequency analysis on the text entries in the data obtained from the original cleaning process.
# For the top 200 words with the highest frequency, we will perform sentiment analysis. Based on their scores in subjectivity and sentiment, we will determine whether they are biased or not.

# Prepare necessary packages
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
import re
from textblob import TextBlob

# Read the Excel file (Clean data)
file_path = 'C:/Users/LC180/Desktop/AI.XPERIENCE/Cleaned_data1.xlsx'
targetFile = pd.read_excel(file_path)

# Extract the target column and convert all entries to strings, handling NaN values
text_data = targetFile['TextEntry'].astype(str).fillna('')

# Combine all text entries into a single string
combined_text = " ".join(text_data)

# Get the stop words from NLTK
stop_words = set(stopwords.words('english'))

# Function to preprocess text using the NLTK stop words list
def preprocess_text_nltk(text):
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text)
    words = word_tokenize(text)
    filtered_words = [word for word in words if word not in stop_words]
    return filtered_words

# Apply the preprocessing function to the text entries
targetFile['Processed_Text'] = targetFile['TextEntry'].astype(str).fillna('').apply(preprocess_text_nltk)

# Flatten the list of tokens and calculate word frequencies
all_tokens = [token for sublist in targetFile['Processed_Text'] for token in sublist]
word_freq = Counter(all_tokens)

# Display the most common words
most_common_words = word_freq.most_common(300)

# Calculate sentiment scores using TextBlob for the top 300 words
def get_sentiment(text):
    blob = TextBlob(text)
    return blob.sentiment.polarity, blob.sentiment.subjectivity

word_sentiment_subjectivity = []
for word, freq in most_common_words:
    avg_sentiment = 0
    avg_subjectivity = 0
    count = 0
    for text in text_data:
        if word in text:
            sentiment, subjectivity = get_sentiment(text)
            avg_sentiment += sentiment
            avg_subjectivity += subjectivity
            count += 1
    if count > 0:
        avg_sentiment /= count
        avg_subjectivity /= count
    word_sentiment_subjectivity.append((word, avg_sentiment, avg_subjectivity, freq))

# Convert to DataFrame for better visualization
sentiment_subjectivity_df = pd.DataFrame(word_sentiment_subjectivity, columns=['Word', 'Avg_Sentiment', 'Avg_Subjectivity', 'Frequency'])
print(sentiment_subjectivity_df)

# Save the DataFrame to an Excel file
output_file_path = 'C:/Users/LC180/Desktop/AI.XPERIENCE/BiasedWord2.xlsx'
sentiment_subjectivity_df.to_excel(output_file_path, index=False)



