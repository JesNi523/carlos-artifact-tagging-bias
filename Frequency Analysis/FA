# Install the textblob library
!pip install textblob

import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
import re
from nltk.corpus import wordnet as wn
from textblob import TextBlob

# Download necessary NLTK resources
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

# Read the CSV file (Data obtained from sentiment analysis)
file_path = '/Users/Sophia/Desktop/Datasets/AIXperience/clean_data.csv'
Sentiment_ana_result = pd.read_csv(file_path)

# Extract the target column
text_data = Sentiment_ana_result['TextEntry']

# Combine all text entries into a single string
combined_text = " ".join(text_data.dropna().astype(str))

# Get the stop words from NLTK
stop_words = set(stopwords.words('english'))

# Function to preprocess text using the NLTK stop words list
def preprocess_text_nltk(text):
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text)
    words = word_tokenize(text)
    filtered_words = [word for word in words if word not in stop_words]
    return filtered_words

# Apply the preprocessing function to the text entries
Sentiment_ana_result['Processed_Text'] = Sentiment_ana_result['TextEntry'].dropna().astype(str).apply(preprocess_text_nltk)

# Flatten the list of tokens and calculate word frequencies
all_tokens = [token for sublist in Sentiment_ana_result['Processed_Text'] if isinstance(sublist, list) for token in sublist]
word_freq = Counter(all_tokens)

# Display the most common words
most_common_words = word_freq.most_common(200)  # Get the top 200 common words
print(most_common_words)

# Function to filter sentiment-laden words using part-of-speech tagging
def is_sentiment_word(word):
    synsets = wn.synsets(word)
    if synsets:
        pos = synsets[0].pos()
        return pos in ['a', 's', 'r', 'v']  # Adjectives, Adverbs, Verbs
    return False

# Filter the most common words to keep only sentiment-laden words
sentiment_words = [word for word in most_common_words if is_sentiment_word(word[0])]

# Add sentiment analysis using TextBlob
Sentiment_ana_result['Sentiment_TextBlob'] = Sentiment_ana_result['TextEntry'].dropna().astype(str).apply(lambda x: TextBlob(x).sentiment.polarity)
Sentiment_ana_result['Subjectivity_TextBlob'] = Sentiment_ana_result['TextEntry'].dropna().astype(str).apply(lambda x: TextBlob(x).sentiment.subjectivity)

# Calculate the average sentiment and subjectivity for each word in the sentiment words list
word_sentiment_subjectivity = []
for word, freq in sentiment_words:
    word_data = Sentiment_ana_result[Sentiment_ana_result['Processed_Text'].apply(lambda x: isinstance(x, list) and word in x)]
    avg_sentiment = word_data['Sentiment_TextBlob'].mean()
    avg_subjectivity = word_data['Subjectivity_TextBlob'].mean()
    word_sentiment_subjectivity.append((word, avg_sentiment, avg_subjectivity, freq))

# Convert to DataFrame for better visualization
sentiment_subjectivity_df = pd.DataFrame(word_sentiment_subjectivity, columns=['Word', 'Avg_Sentiment', 'Avg_Subjectivity', 'Frequency'])
print(sentiment_subjectivity_df)

# Save the DataFrame to a CSV file
output_file_path = '/Users/Sophia/Desktop/Datasets/AIXperience/sentiment_data.csv'
sentiment_subjectivity_df.to_csv(output_file_path, index=False)

import pandas as pd
import nltk
from nltk.corpus import stopwords, words
from nltk.tokenize import word_tokenize
from collections import Counter
import re
from textblob import TextBlob
import string

# Download necessary NLTK resources
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('words')

# Initialize the English words set
english_words = set(words.words())

# Read the CSV file
file_path = '/Users/Sophia/Desktop/Datasets/AIXperience/clean_data.csv'
targetFile = pd.read_csv(file_path)

# Display the first few rows of the dataframe
print(targetFile.head())

# Extract the target column and convert all entries to strings, handling NaN values
text_data = targetFile['TextEntry'].astype(str).fillna('')

# Get the stop words from NLTK
stop_words = set(stopwords.words('english'))

def is_valid_word(word):
    return word in english_words or (word.isupper() and len(word) > 1)

def preprocess_text_nltk(text):
    text = text.lower()  # Convert text to lowercase
    text = re.sub(r'[^a-z\s]', '', text)  # Remove non-alphabet characters
    words = word_tokenize(text)  # Tokenize text into words
    filtered_words = []
    for word in words:
        # Check if the word is not a standalone letter, not a stopword, and is a valid word
        if len(word) > 1 and word not in stop_words and is_valid_word(word):
            filtered_words.append(word)
    return filtered_words

# Apply the preprocessing function to the text entries
targetFile['Processed_Text'] = targetFile['TextEntry'].astype(str).fillna('').apply(preprocess_text_nltk)

# Display the first few rows of the dataframe to confirm preprocessing
print(targetFile[['TextEntry', 'Processed_Text']].head())

# Calculate word frequencies
word_freq = Counter([word for words in targetFile['Processed_Text'] for word in words])

# Calculate total number of tokens
total_tokens = sum(word_freq.values())

# Calculate word frequencies as proportions
word_proportions = {word: freq / total_tokens for word, freq in word_freq.items()}

# Sort words by frequency
sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)

# Display the most common words with proportions
print("Most common words with frequencies and proportions:")
for word, freq in sorted_words[:300]:  # Get the top 300 common words
    prop = word_proportions[word]
    print(f"Word: {word}, Frequency: {freq}, Proportion: {prop:.4f}")

# Create DataFrame with word frequencies and proportions
df_word_freq = pd.DataFrame(sorted_words[:300], columns=['Word', 'Frequency'])
df_word_freq['Proportion'] = df_word_freq['Frequency'] / total_tokens

# Save to CSV file
output_file = '/Users/Sophia/Desktop/Datasets/AIXperience/frequency_data.csv'
df_word_freq.to_csv(output_file, index=False)

print(f"Frequency data saved to '{output_file}'")
